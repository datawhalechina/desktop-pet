import os
import json
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse, StreamingResponse
from dotenv import load_dotenv
from openai import OpenAI
import asyncio
from typing import Dict, Any

load_dotenv()

app = FastAPI()

# åˆ›å»ºOpenAIå®¢æˆ·ç«¯ç”¨äºè½¬å‘
upstream_client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url=os.getenv("OPENAI_API_URL")
)

def log_request(body: Dict[str, Any]):
    """è®°å½•è¯·æ±‚ä¿¡æ¯"""
    print("=" * 60)
    print("ğŸ“¨ æ”¶åˆ°OpenAIè¯·æ±‚:")
    print(json.dumps(body, indent=2, ensure_ascii=False))
    print(f"ğŸ”§ æ¨¡å‹: {body.get('model', 'default')}")
    print(f"ğŸ’¬ æ¶ˆæ¯æ•°é‡: {len(body.get('messages', []))}")
    print(f"ğŸŒŠ æµå¼æ¨¡å¼: {body.get('stream', False)}")
    print("=" * 60)

async def stream_generator(stream_response):
    """å¼‚æ­¥ç”Ÿæˆå™¨å¤„ç†æµå¼å“åº”"""
    try:
        for chunk in stream_response:
            chunk_data = chunk.model_dump()
            print(f"ğŸ“¡ æµå¼æ•°æ®å—: {chunk_data}")
            yield f"data: {json.dumps(chunk_data)}\n\n"
        yield "data: [DONE]\n\n"
    except Exception as e:
        print(f"âŒ æµå¼å¤„ç†é”™è¯¯: {str(e)}")
        error_chunk = {
            "error": {
                "message": f"Stream error: {str(e)}",
                "type": "stream_error"
            }
        }
        yield f"data: {json.dumps(error_chunk)}\n\n"

@app.post("/v1/chat/completions")
async def openai_proxy(request: Request):
    try:
        # è·å–è¯·æ±‚ä½“
        body = await request.json()
        log_request(body)
        
        # æå–æ‰€æœ‰å¯èƒ½çš„å‚æ•°
        api_params = {
            "model": body.get("model", os.getenv("OPENAI_MODEL")),
            "messages": body.get("messages", []),
        }
        
        # å¯é€‰å‚æ•°ï¼Œåªæœ‰åœ¨è¯·æ±‚ä¸­å­˜åœ¨æ—¶æ‰æ·»åŠ 
        optional_params = [
            "temperature", "max_tokens", "top_p", "frequency_penalty", 
            "presence_penalty", "stop", "n", "stream", "logit_bias",
            "user", "response_format", "seed", "tools", "tool_choice"
        ]
        
        for param in optional_params:
            if param in body:
                api_params[param] = body[param]
        
        print(f"ğŸš€ è½¬å‘å‚æ•°: {list(api_params.keys())}")
        
        # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è°ƒç”¨ä¸Šæ¸¸API
        chat_completion = upstream_client.chat.completions.create(**api_params)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæµå¼å“åº”
        if body.get("stream", False):
            print("ğŸŒŠ å¼€å§‹æµå¼å“åº”")
            return StreamingResponse(
                stream_generator(chat_completion),
                media_type="text/plain",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "Content-Type": "text/plain; charset=utf-8"
                }
            )
        else:
            # éæµå¼å“åº”
            result = chat_completion.model_dump()
            
            print("âœ… APIå“åº”æˆåŠŸ:")
            print(f"ğŸ“ å†…å®¹é¢„è§ˆ: {result.get('choices', [{}])[0].get('message', {}).get('content', '')[:100]}...")
            print(f"ğŸ“Š ä½¿ç”¨æƒ…å†µ: {result.get('usage', {})}")
            
            return JSONResponse(content=result)
        
    except Exception as e:
        print(f"âŒ è°ƒç”¨å¼‚å¸¸: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={
                "error": {
                    "message": f"Proxy error: {str(e)}",
                    "type": "proxy_error"
                }
            }
        )

# æ·»åŠ å…¶ä»–OpenAIç«¯ç‚¹çš„è½¬å‘æ”¯æŒ
@app.get("/v1/models")
async def list_models():
    """è½¬å‘æ¨¡å‹åˆ—è¡¨è¯·æ±‚"""
    try:
        print("ğŸ“‹ æ”¶åˆ°æ¨¡å‹åˆ—è¡¨è¯·æ±‚")
        models = upstream_client.models.list()
        result = models.model_dump()
        print(f"ğŸ“‹ è¿”å› {len(result.get('data', []))} ä¸ªæ¨¡å‹")
        return JSONResponse(content=result)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ—è¡¨è·å–å¼‚å¸¸: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={
                "error": {
                    "message": f"Models list error: {str(e)}",
                    "type": "models_error"
                }
            }
        )

@app.post("/v1/embeddings")
async def embeddings_proxy(request: Request):
    """è½¬å‘åµŒå…¥è¯·æ±‚"""
    try:
        body = await request.json()
        print("ğŸ”¤ æ”¶åˆ°åµŒå…¥è¯·æ±‚:")
        print(json.dumps(body, indent=2, ensure_ascii=False))
        
        # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è°ƒç”¨åµŒå…¥API
        embedding_response = upstream_client.embeddings.create(**body)
        result = embedding_response.model_dump()
        
        print(f"âœ… åµŒå…¥å“åº”æˆåŠŸ: {len(result.get('data', []))} ä¸ªå‘é‡")
        return JSONResponse(content=result)
        
    except Exception as e:
        print(f"âŒ åµŒå…¥è¯·æ±‚å¼‚å¸¸: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={
                "error": {
                    "message": f"Embeddings error: {str(e)}",
                    "type": "embeddings_error"
                }
            }
        )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
